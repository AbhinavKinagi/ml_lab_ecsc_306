{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementaion of Logic Gates using Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Logic Gates to implement for 3 i/p variables.\n",
    "\n",
    "\n",
    "1. OR\n",
    "2. NOT\n",
    "3. XOR\n",
    "4. XNOR\n",
    "5. NAND\n",
    "6. NOR\n",
    "7. AND\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Perceptron model function\n",
    "\n",
    "$$ z = \\sum_{i=1}^m(w_i * x_i) +b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sigmoid Function\n",
    "\n",
    "$$ f(z) =  \\frac{1}{1 + e^{-z}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Part 1.\n",
    "Write a tensorflow program to implement a perceptron using functions described above for AND , OR and NAND logic gates. \n",
    "Choose weights and bias values accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AND gate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=  0 cost=  0.876559\n",
      "iteration=  1000 cost=  0.529647\n",
      "iteration=  2000 cost=  0.393386\n",
      "iteration=  3000 cost=  0.317832\n",
      "iteration=  4000 cost=  0.269514\n",
      "iteration=  5000 cost=  0.235263\n",
      "iteration=  6000 cost=  0.209296\n",
      "iteration=  7000 cost=  0.188717\n",
      "iteration=  8000 cost=  0.171903\n",
      "iteration=  9000 cost=  0.157857\n",
      "iteration=  10000 cost=  0.145922\n",
      "iteration=  11000 cost=  0.135643\n",
      "iteration=  12000 cost=  0.126692\n",
      "Validating output for AND GATE\n",
      "[[ 0.00567448]\n",
      " [ 0.13340551]\n",
      " [ 0.13559233]\n",
      " [ 0.80884409]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "x=tf.placeholder(tf.float32,shape=[None,2])\n",
    "y=tf.placeholder(tf.float32,shape=[None,1])\n",
    "\n",
    "weights=tf.Variable(tf.random_normal([2,1]),dtype=tf.float32)\n",
    "bias=tf.Variable(tf.random_normal([1]),dtype=tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "multiply1=tf.add(tf.matmul(x,weights),bias)\n",
    "z=tf.nn.sigmoid(multiply1)\n",
    "\n",
    "\n",
    "cost=tf.reduce_mean((y*tf.log(z)+(1-y)*tf.log(1-z))*-1)\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "\n",
    "inp=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "op=np.array([[0],[0],[0],[1]])\n",
    "with tf.Session() as sess:\n",
    "   \n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(12001):\n",
    "        res,_=sess.run([cost,optimizer],feed_dict={x:inp,y:op})\n",
    "        if i%1000==0:\n",
    "            print (\"iteration= \",i,\"cost= \",res)\n",
    "    print (\"Validating output for AND GATE\")\n",
    "    result=sess.run(z,feed_dict={x:inp})\n",
    "    print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OR gate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=  0 cost=  1.47753\n",
      "iteration=  1000 cost=  0.394091\n",
      "iteration=  2000 cost=  0.29855\n",
      "iteration=  3000 cost=  0.238516\n",
      "iteration=  4000 cost=  0.197504\n",
      "iteration=  5000 cost=  0.167914\n",
      "iteration=  6000 cost=  0.145639\n",
      "iteration=  7000 cost=  0.128309\n",
      "iteration=  8000 cost=  0.114475\n",
      "iteration=  9000 cost=  0.103196\n",
      "iteration=  10000 cost=  0.0938411\n",
      "iteration=  11000 cost=  0.0859673\n",
      "iteration=  12000 cost=  0.0792568\n",
      "Validating output for OR GATE\n",
      "[[ 0.16627915]\n",
      " [ 0.9340021 ]\n",
      " [ 0.93621552]\n",
      " [ 0.99904078]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "x=tf.placeholder(tf.float32,shape=[None,2])\n",
    "y=tf.placeholder(tf.float32,shape=[None,1])\n",
    "\n",
    "weights=tf.Variable(tf.random_normal([2,1]),dtype=tf.float32)\n",
    "bias=tf.Variable(tf.random_normal([1]),dtype=tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "multiply1=tf.add(tf.matmul(x,weights),bias)\n",
    "z=tf.nn.sigmoid(multiply1)\n",
    "\n",
    "\n",
    "cost=tf.reduce_mean((y*tf.log(z)+(1-y)*tf.log(1-z))*-1)\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "\n",
    "inp=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "op=np.array([[0],[1],[1],[1]])\n",
    "with tf.Session() as sess:\n",
    "   \n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(12001):\n",
    "        res,_=sess.run([cost,optimizer],feed_dict={x:inp,y:op})\n",
    "        if i%1000==0:\n",
    "            print (\"iteration= \",i,\"cost= \",res)\n",
    "    print (\"Validating output for OR GATE\")\n",
    "    result=sess.run(z,feed_dict={x:inp})\n",
    "    print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAND gate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=  0 cost=  0.746466\n",
      "iteration=  1000 cost=  0.467316\n",
      "iteration=  2000 cost=  0.365958\n",
      "iteration=  3000 cost=  0.303616\n",
      "iteration=  4000 cost=  0.260828\n",
      "iteration=  5000 cost=  0.229254\n",
      "iteration=  6000 cost=  0.20478\n",
      "iteration=  7000 cost=  0.18514\n",
      "iteration=  8000 cost=  0.16897\n",
      "iteration=  9000 cost=  0.155395\n",
      "iteration=  10000 cost=  0.143819\n",
      "iteration=  11000 cost=  0.133822\n",
      "iteration=  12000 cost=  0.125098\n",
      "Validating output for NAND GATE\n",
      "[[ 0.99454641]\n",
      " [ 0.8668493 ]\n",
      " [ 0.8671338 ]\n",
      " [ 0.18896069]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "x=tf.placeholder(tf.float32,shape=[None,2])\n",
    "y=tf.placeholder(tf.float32,shape=[None,1])\n",
    "\n",
    "weights=tf.Variable(tf.random_normal([2,1]),dtype=tf.float32)\n",
    "bias=tf.Variable(tf.random_normal([1]),dtype=tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "multiply1=tf.add(tf.matmul(x,weights),bias)\n",
    "z=tf.nn.sigmoid(multiply1)\n",
    "\n",
    "\n",
    "cost=tf.reduce_mean((y*tf.log(z)+(1-y)*tf.log(1-z))*-1)\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "\n",
    "inp=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "op=np.array([[1],[1],[1],[0]])\n",
    "with tf.Session() as sess:\n",
    "   \n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(12001):\n",
    "        res,_=sess.run([cost,optimizer],feed_dict={x:inp,y:op})\n",
    "        if i%1000==0:\n",
    "            print (\"iteration= \",i,\"cost= \",res)\n",
    "    print (\"Validating output for NAND GATE\")\n",
    "    result=sess.run(z,feed_dict={x:inp})\n",
    "    print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Part 2. \n",
    "1. Use tensorflow library functions (Logical/Linear Regression )to find optimal weights and bias for all above mentioned gates.\n",
    "2. Discuss the values choosen for epoch and stopping convergence. Plot the graph for accuracy v/s epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-6f80c2c12783>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# Step 8: train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# run 100 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m           \u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# creating the weight and bias.\n",
    "# The defined variables will be initialized to zero.\n",
    "W = tf.Variable(0.0, name=\"weights\")\n",
    "b = tf.Variable(0.0, name=\"bias\")\n",
    "\n",
    "\n",
    "#  Creating placeholders for input X and label Y.\n",
    "def inputs():\n",
    "    \"\"\"\n",
    "    Defining the place_holders.\n",
    "    :return:\n",
    "            Returning the data and label lace holders.\n",
    "    \"\"\"\n",
    "    X = tf.placeholder(tf.float32, name=\"X\")\n",
    "    Y = tf.placeholder(tf.float32, name=\"Y\")\n",
    "    return X,Y\n",
    "\n",
    "# Create the prediction.\n",
    "def inference(X):\n",
    "    \"\"\"\n",
    "    Forward passing the X.\n",
    "    :param X: Input.\n",
    "    :return: X*W + b.\n",
    "    \"\"\"\n",
    "    return X * W + b\n",
    "\n",
    "def loss(X, Y):\n",
    "    '''\n",
    "    compute the loss by comparing the predicted value to the actual label.\n",
    "    :param X: The input.\n",
    "    :param Y: The label.\n",
    "    :return: The loss over the samples.\n",
    "    '''\n",
    "\n",
    "    # Making the prediction.\n",
    "    Y_predicted = inference(X)\n",
    "    return tf.squared_difference(Y, Y_predicted)\n",
    "\n",
    "\n",
    "# The training function.\n",
    "def train(loss):\n",
    "    learning_rate = 0.0001\n",
    "    return tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Initialize the variables[w and b].\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Get the input tensors\n",
    "    X, Y = inputs()\n",
    "\n",
    "    # Return the train loss and create the train_op.\n",
    "    train_loss = loss(X, Y)\n",
    "    train_op = train(train_loss)\n",
    "\n",
    "    # Step 8: train the model\n",
    "    for epoch_num in range(num_epochs): # run 100 epochs\n",
    "        for x, y in data:\n",
    "          train_op = train(train_loss)\n",
    "\n",
    "          # Session runs train_op to minimize loss\n",
    "          loss_value,_ = sess.run([train_loss,train_op], feed_dict={X: x, Y: y})\n",
    "\n",
    "        # Displaying the loss per epoch.\n",
    "        print('epoch %d, loss=%f' %(epoch_num+1, loss_value))\n",
    "\n",
    "        # save the values of weight and bias\n",
    "        wcoeff, bias = sess.run([W, b])\n",
    "\n",
    "Input_values = data[:,0]\n",
    "Labels = data[:,1]\n",
    "Prediction_values = data[:,0] * wcoeff + bias\n",
    "plt.plot(Input_values, Labels, 'ro', label='main')\n",
    "plt.plot(Input_values, Prediction_values, label='Predicted')\n",
    "\n",
    "# Saving the result.\n",
    "plt.legend()\n",
    "plt.savefig('plot.png')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
